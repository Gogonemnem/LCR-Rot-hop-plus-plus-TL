
@inproceedings{Chen2020,
	title = {Recall and {Learn}: {Fine}-tuning {Deep} {Pretrained} {Language} {Models} with {Less} {Forgetting}},
	abstract = {Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance. To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks. Specifically, we propose a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually. Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark. Our method also enables BERT-base to achieve better performance than directly fine-tuning of BERT-large. Further, we provide the open-source RecAdam optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community.},
	urldate = {2022-04-17},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP} 2020)},
	publisher = {ACL},
	author = {Chen, Sanyuan and Hou, Yutai and Cui, Yiming and Che, Wanxiang and Liu, Ting and Yu, Xiangzhan},
	year = {2020},
	keywords = {Computer Science - Computation and Language},
}

@article{Li2018,
	title = {Hyperband: {A} {Novel} {Bandit}-{Based} {Approach} to {Hyperparameter} {Optimization}},
	volume = {18},
	shorttitle = {Hyperband},
	abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
	number = {1},
	urldate = {2022-04-15},
	journal = {Journal of Machine Learning Research},
	author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {6765--6816},
}

@inproceedings{Devlin2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-03-09},
	booktitle = {17th {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({NAACL}-{HLT} 2019)},
	publisher = {ACL},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
	keywords = {Computer Science - Computation and Language},
	pages = {4171--4186},
}

@inproceedings{Subramanian2018,
	title = {Learning {General} {Purpose} {Distributed} {Sentence} {Representations} via {Large} {Scale} {Multi}-task {Learning}},
	abstract = {A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. We train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations.},
	urldate = {2022-03-21},
	booktitle = {6th {International} {Conference} on {Learning} {Representation} ({ICLR} 2018)},
	publisher = {OpenReview.net},
	author = {Subramanian, Sandeep and Trischler, Adam and Bengio, Yoshua and Pal, Christopher J.},
	year = {2018},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{Trusca2020,
	series = {{LNCS}},
	title = {A {Hybrid} {Approach} for {Aspect}-{Based} {Sentiment} {Analysis} {Using} {Deep} {Contextual} {Word} {Embeddings} and {Hierarchical} {Attention}},
	volume = {12128},
	language = {en},
	urldate = {2022-03-06},
	booktitle = {20th {International} {Conference} on {Web} {Engineering} ({ICWE} 2020)},
	publisher = {Springer},
	author = {Truşcǎ, Maria Mihaela and Wassenberg, Daan and Frasincar, Flavius and Dekker, Rommert},
	year = {2020},
	pages = {365--380},
}

@inproceedings{Schouten2017,
	series = {{LNCS}},
	title = {Ontology-{Enhanced} {Aspect}-{Based} {Sentiment} {Analysis}},
	volume = {10360},
	abstract = {With many people freely expressing their opinions and feelings on the Web, much research has gone into modeling and monetizing opinionated, and usually unstructured and textual, Web-based content. Aspect-based sentiment analysis aims to extract the fine-grained topics, or aspects, that people are talking about, together with the sentiment expressed on those aspects. This allows for a detailed analysis of the sentiment expressed in, for instance, product and service reviews. In this work we focus on knowledge-driven solutions that aim to complement standard machine learning methods. By encoding common domain knowledge into a knowledge repository, or ontology, we are able to exploit this information to improve classification performance for both aspect detection and aspect sentiment analysis. For aspect detection, the ontology-enhanced method needs only 20\% of the training data to achieve results comparable with a standard bag-of-words approach that uses all training data.},
	language = {en},
	booktitle = {17th {International} {Conference} on {Web} {Engineering} ({ICWE} 2017)},
	publisher = {Springer},
	author = {Schouten, Kim and Frasincar, Flavius and de Jong, Franciska},
	year = {2017},
	pages = {302--320},
}

@article{Brauwers2021,
	title = {A {Survey} on {Aspect}-{Based} {Sentiment} {Classification}},
	issn = {0360-0300},
	abstract = {With the constantly growing number of reviews and other sentiment-bearing texts on the Web, the demand for automatic sentiment analysis algorithms continues to expand. Aspect-based sentiment classification (ABSC) allows for the automatic extraction of highly fine-grained sentiment information from text documents or sentences. In this survey, the rapidly evolving state of the research on ABSC is reviewed. A novel taxonomy is proposed that categorizes the ABSC models into three major categories: knowledge-based, machine learning, and hybrid models. This taxonomy is accompanied with summarizing overviews of the reported model performances, and both technical and intuitive explanations of the various ABSC models. State-of-the-art ABSC models are discussed, such as models based on the transformer model, and hybrid deep learning models that incorporate knowledge bases. Additionally, various techniques for representing the model inputs and evaluating the model outputs are reviewed. Furthermore, trends in the research on ABSC are identified and a discussion is provided on the ways in which the field of ABSC can be advanced in the future.},
	urldate = {2022-04-15},
	journal = {ACM Computing Surveys},
	author = {Brauwers, Gianni and Frasincar, Flavius},
	year = {2021},
	keywords = {aspect-based sentiment classification, attention models, deep learning, hybrid models, knowledge-based models},
}

@inproceedings{Wallaart2019,
	title = {A {Hybrid} {Approach} for {Aspect}-{Based} {Sentiment} {Analysis} {Using} a {Lexicalized} {Domain} {Ontology} and {Attentional} {Neural} {Models}},
	volume = {11503},
	isbn = {9783030213473 9783030213480},
	language = {en},
	urldate = {2022-03-06},
	booktitle = {16th {Extended} {Semantic} {Web} {Conference} ({ESWB} 2019)},
	publisher = {Springer},
	author = {Wallaart, Olaf and Frasincar, Flavius},
	year = {2019},
	pages = {363--378},
}

@inproceedings{Tang2015,
	title = {Learning {Semantic} {Representations} of {Users} and {Products} for {Document} {Level} {Sentiment} {Classification}},
	language = {en},
	urldate = {2022-03-07},
	booktitle = {53rd {Meeting} of the {Association} for {Computational} {Linguistics} ({ACL} 2015)},
	publisher = {ACL},
	author = {Tang, Duyu and Qin, Bing and Liu, Ting},
	year = {2015},
	pages = {1014--1023},
}

@inproceedings{Hu2004,
	title = {Mining and {Summarizing} {Customer} {Reviews}},
	language = {en},
	urldate = {2022-03-06},
	booktitle = {10th {ACM} {SIGKDD} international conference on {Knowledge} discovery and {Data} {Mining} ({KDD} 2004)},
	publisher = {ACM},
	author = {Hu, Minqing and Liu, Bing},
	year = {2004},
	pages = {168--177},
}

@inproceedings{Cambria2018,
	title = {{SenticNet} 6: {Ensemble} {Application} of {Symbolic} and {Subsymbolic} {AI} for {Sentiment} {Analysis}},
	copyright = {Copyright (c)},
	shorttitle = {{SenticNet} 5},
	abstract = {With the recent development of deep learning, research in AI has gained new vigor and prominence. While machine learning has succeeded in revitalizing many research fields, such as computer vision, speech recognition, and medical diagnosis, we are yet to witness impressive progress in natural language understanding. One of the reasons behind this unmatched expectation is that, while a bottom-up approach is feasible for pattern recognition, reasoning and understanding often require a top-down approach. In this work, we couple sub-symbolic and symbolic AI to automatically discover conceptual primitives from text and link them to commonsense concepts and named entities in a new three-level knowledge representation for sentiment analysis. In particular, we employ recurrent neural networks to infer primitives by lexical substitution and use them for grounding common and commonsense knowledge by means of multi-dimensional scaling.},
	language = {en},
	urldate = {2022-03-09},
	booktitle = {29th {Conference} on {Information} and {Knowledge} {Management} ({CIKM} 2020)},
	publisher = {ACM},
	author = {Cambria, Erik and Li, Yang and Xing, Frank Z. and Poria, Soujanya},
	year = {2020},
	keywords = {SenticNet},
	pages = {105--114},
}

@inproceedings{Caruana1993,
	title = {Multitask {Learning}: {A} {Knowledge}-{Based} {Source} of {Inductive} {Bias}},
	shorttitle = {Multitask {Learning}},
	abstract = {This paper suggests that it may be easier to learn several hard tasks at one time than to learn these same tasks separately. In effect, the information provided by the training signal for each task serves as a domain-specific inductive bias for the other tasks. Frequentlythe worldgives us clusters of related tasks to learn. When it does not, it is often straightforward to create additional tasks. For many domains, acquiring inductive bias by collecting additional teaching signal may be more practical than the traditional approach of codifying domain-specific biases acquired from human expertise. We call this approach MultitaskLearning (MTL). Since much of the power of an inductive learner follows directly from its inductive bias, multitask learning may yield more powerful learning. An empirical example of multitask connectionist learning is presented where learning improves by training one network on several related tasks at the same time. Multitask decision tree induction is also outl...},
	booktitle = {10th {International} {Conference} on {Machine} {Learning} ({ICML} 1993)},
	publisher = {Morgan Kaufmann},
	author = {Caruana, Richard},
	year = {1993},
	pages = {41--48},
}

@phdthesis{Ruder2019,
	address = {Galway},
	type = {{PhD} thesis},
	title = {Neural {Transfer} {Learning} for {Natural} {Language} {Processing}},
	abstract = {The blue social bookmark and publication sharing system.},
	urldate = {2022-03-10},
	school = {National University of Ireland},
	author = {Ruder, Sebastian},
	year = {2019},
}

@book{Liu2020,
	edition = {2nd},
	title = {Sentiment {Analysis}: {Mining} {Opinions}, {Sentiments}, and {Emotions}},
	isbn = {9781108787284},
	abstract = {Sentiment analysis is the computational study of people's opinions, sentiments, emotions, moods, and attitudes. This fascinating problem offers numerous research challenges, but promises insight useful to anyone interested in opinion analysis and social media analysis. This comprehensive introduction to the topic takes a natural-language-processing point of view to help readers understand the underlying structure of the problem and the language constructs commonly used to express opinions, sentiments, and emotions. The book covers core areas of sentiment analysis and also includes related topics such as debate analysis, intention mining, and fake-opinion detection. It will be a valuable resource for researchers and practitioners in natural language processing, computer science, management sciences, and the social sciences. In addition to traditional computational methods, this second edition includes recent deep learning methods to analyze and summarize sentiments and opinions, and also new material on emotion and mood analysis techniques, emotion-enhanced dialogues, and multimodal emotion analysis.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Liu, Bing},
	year = {2020},
	keywords = {Business \& Economics / Advertising \& Promotion, Computers / Artificial Intelligence / General, Computers / Artificial Intelligence / Natural Language Processing, Computers / Data Science / Data Analytics, Computers / System Administration / Storage \& Retrieval, Political Science / Political Process / General, Psychology / Emotions, Reference / Research, Social Science / Media Studies},
}

@inproceedings{Pang2002,
	title = {Thumbs up? {Sentiment} {Classification} using {Machine} {Learning} {Techniques}},
	shorttitle = {Thumbs up?},
	abstract = {We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.},
	urldate = {2022-03-09},
	booktitle = {2002 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP} 2002)},
	publisher = {ACL},
	author = {Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
	year = {2002},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.6, I.2.7},
	pages = {79--86},
}

@article{Schouten2016,
	title = {Survey on {Aspect}-{Level} {Sentiment} {Analysis}},
	volume = {28},
	number = {3},
	urldate = {2022-03-06},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Schouten, Kim and Frasincar, Flavius},
	year = {2016},
	pages = {813--830},
}

@article{Zaremba2015,
	title = {Learning to {Execute}},
	abstract = {Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of LSTMs in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditionally been seen as too complex for neural networks. We consider a simple class of programs that can be evaluated with a single left-to-right pass using constant memory. Our main result is that LSTMs can learn to map the character-level representations of such programs to their correct outputs. Notably, it was necessary to use curriculum learning, and while conventional curriculum learning proved ineffective, we developed a new variant of curriculum learning that improved our networks' performance in all experimental conditions. The improved curriculum had a dramatic impact on an addition problem, making it possible to train an LSTM to add two 9-digit numbers with 99\% accuracy.},
	urldate = {2022-03-09},
	journal = {arXiv preprint arXiv:1410.4615},
	author = {Zaremba, Wojciech and Sutskever, Ilya},
	year = {2015},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{Zhu2009,
	title = {Multi-aspect opinion polling from textual reviews},
	isbn = {9781605585123},
	abstract = {This paper presents an unsupervised approach to aspect-based opinion polling from raw textual reviews without explicit ratings. The key contribution of this paper is three-fold. First, a multi-aspect bootstrapping algorithm is proposed to learn from unlabeled data aspect-related terms of each aspect to be used for aspect identification. Second, an unsupervised segmentation model is proposed to address the challenge of identifying multiple single-aspect units in a multi-aspect sentence. Finally, an aspect-based opinion polling algorithm is presented. Experiments on real Chinese restaurant reviews show that our opinion polling method can achieve 75.5\% precision performance.},
	urldate = {2022-03-08},
	booktitle = {18th {ACM} conference on {Information} and {Knowledge} {Management} ({CIKM} 2009)},
	publisher = {ACM},
	author = {Zhu, Jingbo and Wang, Huizhen and Tsou, Benjamin K. and Zhu, Muhua},
	year = {2009},
	keywords = {opinion polling, review mining, sentiment analysis},
	pages = {1799--1802},
}

@article{Zheng2018,
	title = {Left-{Center}-{Right} {Separated} {Neural} {Network} for {Aspect}-based {Sentiment} {Analysis} with {Rotatory} {Attention}},
	abstract = {Deep learning techniques have achieved success in aspect-based sentiment analysis in recent years. However, there are two important issues that still remain to be further studied, i.e., 1) how to efficiently represent the target especially when the target contains multiple words; 2) how to utilize the interaction between target and left/right contexts to capture the most important words in them. In this paper, we propose an approach, called left-center-right separated neural network with rotatory attention (LCR-Rot), to better address the two problems. Our approach has two characteristics: 1) it has three separated LSTMs, i.e., left, center and right LSTMs, corresponding to three parts of a review (left context, target phrase and right context); 2) it has a rotatory attention mechanism which models the relation between target and left/right contexts. The target2context attention is used to capture the most indicative sentiment words in left/right contexts. Subsequently, the context2target attention is used to capture the most important word in the target. This leads to a two-side representation of the target: left-aware target and right-aware target. We compare our approach on three benchmark datasets with ten related methods proposed recently. The results show that our approach significantly outperforms the state-of-the-art techniques.},
	urldate = {2022-03-06},
	journal = {arXiv preprint arXiv:1802.00892},
	author = {Zheng, Shiliang and Xia, Rui},
	month = feb,
	year = {2018},
	keywords = {Computer Science - Computation and Language},
}

@article{Xu2019,
	title = {Sentiment {Analysis} of {Comment} {Texts} {Based} on {BiLSTM}},
	volume = {7},
	issn = {2169-3536},
	urldate = {2022-03-06},
	journal = {IEEE Access},
	author = {Xu, Guixian and Meng, Yueting and Qiu, Xiaoyu and Yu, Ziheng and Wu, Xu},
	year = {2019},
	pages = {51522--51532},
}

@article{Wu2019,
	title = {Aspect-based sentiment analysis via fusing multiple sources of textual knowledge},
	volume = {183},
	issn = {0950-7051},
	abstract = {The aim of aspect-based sentiment analysis (ABSA) is to predict sentiment polarity of text toward a specific aspect. Although existing neural network models show promising performances on ABSA, their capabilities can be unsatisfactory in cases where the amount of training data is limited. In this paper, we propose a unified model which exploits and incorporates multiple sources of knowledge to improve its ability on ABSA. Structure knowledge is extracted via clause recognition and fused in the model through the generation of multiple context representations to force the model to capture aspect-specific context information. Sentiment knowledge is exploited by means of training a general classification model with the sentiment labels of documents and fused through pretraining specific layers to extract contextual features and predict sentiment polarities more accurately. In addition, information of conjunctions is fused in our model to capture the relations between clauses and provide additional sentiment features. Experimental results on five publicly available ABSA datasets validate the effectiveness of our method and prove that multiple sources of knowledge can collaboratively enhance our model.},
	language = {en},
	urldate = {2022-03-08},
	journal = {Knowledge-Based Systems},
	author = {Wu, Sixing and Xu, Yuanfan and Wu, Fangzhao and Yuan, Zhigang and Huang, Yongfeng and Li, Xing},
	year = {2019},
	keywords = {Aspect-based sentiment analysis, Discourse segmentation, Neural network, Opinion mining},
	pages = {104868},
}

@inproceedings{Tang2016,
	title = {Aspect {Level} {Sentiment} {Classification} with {Deep} {Memory} {Network}},
	abstract = {We introduce a deep memory network for aspect level sentiment classification. Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect. Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation.},
	urldate = {2022-03-10},
	booktitle = {2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP} 2016)},
	publisher = {ACL},
	author = {Tang, Duyu and Qin, Bing and Liu, Ting},
	year = {2016},
	keywords = {Computer Science - Computation and Language},
	pages = {214--222},
}

@inproceedings{SemEval2015,
	title = {{SemEval}-2015 {Task} 12: {Aspect} {Based} {Sentiment} {Analysis}},
	language = {en},
	urldate = {2022-03-07},
	booktitle = {9th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval} 2015)},
	publisher = {ACL},
	author = {Pontiki, Maria and Galanis, Dimitris and Papageorgiou, Haris and Manandhar, Suresh and Androutsopoulos, Ion},
	year = {2015},
	pages = {486--495},
}

@inproceedings{SemEval2016,
	title = {{SemEval}-2016 {Task} 5: {Aspect} {Based} {Sentiment} {Analysis}},
	language = {en},
	urldate = {2022-03-07},
	booktitle = {10th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval} 2016)},
	publisher = {ACL},
	author = {Pontiki, Maria and Galanis, Dimitris and Papageorgiou, Haris and Androutsopoulos, Ion and Manandhar, Suresh and AL-Smadi, Mohammad and Al-Ayyoub, Mahmoud and Zhao, Yanyan and Qin, Bing and De Clercq, Orphee and Hoste, Veronique and Apidianaki, Marianna and Tannier, Xavier and Loukachevitch, Natalia and Kotelnikov, Evgeniy and Bel, Núria and Jiménez-Zafra, Salud María and Eryiğit, Gülşen},
	year = {2016},
	pages = {19--30},
}

@article{pan2010,
	title = {A {Survey} on {Transfer} {Learning}},
	volume = {22},
	issn = {1041-4347},
	number = {10},
	urldate = {2022-03-15},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	year = {2010},
	pages = {1345--1359},
}

@inproceedings{McAuley2015,
	title = {Image-based {Recommendations} on {Styles} and {Substitutes}},
	abstract = {Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on fine-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.},
	urldate = {2022-03-10},
	booktitle = {38th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} on {Information} {Retrieval} ({SIGIR} 2015)},
	publisher = {ACM},
	author = {McAuley, Julian and Targett, Christopher and Shi, Qinfeng and Hengel, Anton van den},
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval},
	pages = {43--52},
}

@article{Graves2005,
	title = {Framewise phoneme classification with bidirectional {LSTM} and other neural network architectures},
	volume = {18},
	issn = {08936080},
	language = {en},
	number = {5-6},
	urldate = {2022-03-15},
	journal = {Neural Networks},
	author = {Graves, Alex and Schmidhuber, Jürgen},
	year = {2005},
	pages = {602--610},
}

@inproceedings{Cho2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	urldate = {2022-03-09},
	booktitle = {2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {ACL},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	year = {2014},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	pages = {1724--1734},
}

@inproceedings{Chikersal2015,
	title = {{SeNTU}: {Sentiment} {Analysis} of {Tweets} by {Combining} a {Rule}-based {Classifier} with {Supervised} {Learning}},
	shorttitle = {{SeNTU}},
	urldate = {2022-03-09},
	booktitle = {9th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval} 2015)},
	publisher = {ACL},
	author = {Chikersal, Prerna and Poria, Soujanya and Cambria, Erik},
	year = {2015},
	pages = {647--651},
}

@inproceedings{Kim2014,
	title = {Convolutional {Neural} {Networks} for {Sentence} {Classification}},
	abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
	urldate = {2022-03-09},
	booktitle = {2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP} 2014)},
	publisher = {ACL},
	author = {Kim, Yoon},
	year = {2014},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	pages = {1746--1751},
}

@article{Hochreiter1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	year = {1997},
	pages = {1735--1780},
}

@inproceedings{He2018,
	title = {Exploiting {Document} {Knowledge} for {Aspect}-level {Sentiment} {Classification}},
	abstract = {Attention-based long short-term memory (LSTM) networks have proven to be useful in aspect-level sentiment classification. However, due to the difficulties in annotating aspect-level data, existing public datasets for this task are all relatively small, which largely limits the effectiveness of those neural models. In this paper, we explore two approaches that transfer knowledge from document- level data, which is much less expensive to obtain, to improve the performance of aspect-level sentiment classification. We demonstrate the effectiveness of our approaches on 4 public datasets from SemEval 2014, 2015, and 2016, and we show that attention-based LSTM benefits from document-level knowledge in multiple ways.},
	urldate = {2022-03-06},
	booktitle = {56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({ACL} 2018)},
	publisher = {ACL},
	author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
	year = {2018},
	keywords = {Computer Science - Computation and Language},
	pages = {579--585},
}

@article{Caruana1997,
	title = {Multitask {Learning}},
	volume = {28},
	issn = {1573-0565},
	abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
	language = {en},
	number = {1},
	urldate = {2022-03-10},
	journal = {Machine Learning},
	author = {Caruana, Rich},
	year = {1997},
	pages = {41--75},
}

@article{Do2019,
	title = {Deep {Learning} for {Aspect}-{Based} {Sentiment} {Analysis}: {A} {Comparative} {Review}},
	volume = {118},
	issn = {0957-4174},
	shorttitle = {Deep {Learning} for {Aspect}-{Based} {Sentiment} {Analysis}},
	abstract = {The increasing volume of user-generated content on the web has made sentiment analysis an important tool for the extraction of information about the human emotional state. A current research focus for sentiment analysis is the improvement of granularity at aspect level, representing two distinct aims: aspect extraction and sentiment classification of product reviews and sentiment classification of target-dependent tweets. Deep learning approaches have emerged as a prospect for achieving these aims with their ability to capture both syntactic and semantic features of text without requirements for high-level feature engineering, as is the case in earlier methods. In this article, we aim to provide a comparative review of deep learning for aspect-based sentiment analysis to place different approaches in context.},
	language = {en},
	urldate = {2022-03-09},
	journal = {Expert Systems with Applications},
	author = {Do, Hai Ha and Prasad, PWC and Maag, Angelika and Alsadoon, Abeer},
	year = {2019},
	pages = {272--299},
}

@article{Collobert2011,
	title = {Natural {Language} {Processing} (almost) from {Scratch}},
	abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
	urldate = {2022-03-09},
	journal = {arXiv:1103.0398 [cs]},
	author = {Collobert, Ronan and Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	year = {2011},
	note = {arXiv: 1103.0398},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{Zhang2021,
	title = {A {Survey} on {Multi}-{Task} {Learning}},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {https://ieeexplore.ieee.org/document/9392366/},
	doi = {10.1109/TKDE.2021.3070203},
	urldate = {2022-03-15},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Yu and Yang, Qiang},
	year = {2021},
	pages = {1--1},
}

@inproceedings{tabish_malware_2009,
	address = {New York, NY, USA},
	series = {{CSI}-{KDD} '09},
	title = {Malware detection using statistical analysis of byte-level file content},
	isbn = {9781605586694},
	url = {https://doi.org/10.1145/1599272.1599278},
	doi = {10.1145/1599272.1599278},
	abstract = {Commercial anti-virus software are unable to provide protection against newly launched (a.k.a "zero-day") malware. In this paper, we propose a novel malware detection technique which is based on the analysis of byte-level file content. The novelty of our approach, compared with existing content based mining schemes, is that it does not memorize specific byte-sequences or strings appearing in the actual file content. Our technique is non-signature based and therefore has the potential to detect previously unknown and zero-day malware. We compute a wide range of statistical and information-theoretic features in a block-wise manner to quantify the byte-level file content. We leverage standard data mining algorithms to classify the file content of every block as normal or potentially malicious. Finally, we correlate the block-wise classification results of a given file to categorize it as benign or malware. Since the proposed scheme operates at the byte-level file content; therefore, it does not require any a priori information about the filetype. We have tested our proposed technique using a benign dataset comprising of six different filetypes --- DOC, EXE, JPG, MP3, PDF and ZIP and a malware dataset comprising of six different malware types --- backdoor, trojan, virus, worm, constructor and miscellaneous. We also perform a comparison with existing data mining based malware detection techniques. The results of our experiments show that the proposed nonsignature based technique surpasses the existing techniques and achieves more than 90\% detection accuracy.},
	urldate = {2022-03-09},
	booktitle = {Proceedings of the {ACM} {SIGKDD} {Workshop} on {CyberSecurity} and {Intelligence} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Tabish, S. Momina and Shafiq, M. Zubair and Farooq, Muddassar},
	month = jun,
	year = {2009},
	keywords = {computer malware, data mining, forensics},
	pages = {23--31},
}

@article{Ma2017,
	title = {Interactive {Attention} {Networks} for {Aspect}-{Level} {Sentiment} {Classification}},
	url = {http://arxiv.org/abs/1709.00893},
	abstract = {Aspect-level sentiment classification aims at identifying the sentiment polarity of specific target in its context. Previous approaches have realized the importance of targets in sentiment classification and developed various methods with the goal of precisely modeling their contexts via generating target-specific representations. However, these studies always ignore the separate modeling of targets. In this paper, we argue that both targets and contexts deserve special treatment and need to be learned their own representations via interactive learning. Then, we propose the interactive attention networks (IAN) to interactively learn attentions in the contexts and targets, and generate the representations for targets and contexts separately. With this design, the IAN model can well represent a target and its collocative context, which is helpful to sentiment classification. Experimental results on SemEval 2014 Datasets demonstrate the effectiveness of our model.},
	urldate = {2022-03-09},
	journal = {arXiv:1709.00893 [cs]},
	author = {Ma, Dehong and Li, Sujian and Zhang, Xiaodong and Wang, Houfeng},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.00893},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{Berger1996,
	title = {A maximum entropy approach to natural language processing},
	volume = {22},
	issn = {0891-2017},
	abstract = {The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.},
	number = {1},
	journal = {Computational Linguistics},
	author = {Berger, Adam L. and Pietra, Vincent J. Della and Pietra, Stephen A. Della},
	month = mar,
	year = {1996},
	pages = {39--71},
}

@article{Nigam1999,
	title = {Using {Maximum} {Entropy} for {Text} {Classification}},
	url = {https://www.semanticscholar.org/paper/Using-Maximum-Entropy-for-Text-Classification-Nigam-Lafferty/656859af2ed88cfa23f2bd063c1816a8fc04c47e},
	abstract = {This paper uses maximum entropy techniques for text classification by estimating the conditional distribution of the class variable given the document by comparing accuracy to naive Bayes and showing that maximum entropy is sometimes significantly better, but also sometimes worse. This paper proposes the use of maximum entropy techniques for text classification. Maximum entropy is a probability distribution estimation technique widely used for a variety of natural language tasks, such as language modeling, part-of-speech tagging, and text segmentation. The underlying principle of maximum entropy is that without external knowledge, one should prefer distributions that are uniform. Constraints on the distribution, derived from labeled training data, inform the technique where to be minimally non-uniform. The maximum entropy formulation has a unique solution which can be found by the improved iterative scaling algorithm. In this paper, maximum entropy is used for text classification by estimating the conditional distribution of the class variable given the document. In experiments on several text datasets we compare accuracy to naive Bayes and show that maximum entropy is sometimes significantly better, but also sometimes worse. Much future work remains, but the results indicate that maximum entropy is a promising technique for text classification.},
	language = {en},
	urldate = {2022-03-09},
	journal = {undefined},
	author = {Nigam, K. and Lafferty, J. and McCallum, A.},
	year = {1999},
}

@inproceedings{Joachims1998,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Text categorization with {Support} {Vector} {Machines}: {Learning} with many relevant features},
	isbn = {9783540697817},
	shorttitle = {Text categorization with {Support} {Vector} {Machines}},
	doi = {10.1007/BFb0026683},
	abstract = {This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning.},
	language = {en},
	booktitle = {Machine {Learning}: {ECML}-98},
	publisher = {Springer},
	author = {Joachims, Thorsten},
	editor = {Nédellec, Claire and Rouveirol, Céline},
	year = {1998},
	keywords = {Irrelevant Feature , Linear Threshold Function , Radial Basic Function , Support Vector Machine , Text Categorization },
	pages = {137--142},
}

@article{Domingos1997,
	title = {On the {Optimality} of the {Simple} {Bayesian} {Classifier} under {Zero}-{One} {Loss}},
	volume = {29},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1007413511361},
	doi = {10.1023/A:1007413511361},
	abstract = {The simple Bayesian classifier is known to be optimal when attributes are independent given the class, but the question of whether other sufficient conditions for its optimality exist has so far not been explored. Empirical results showing that it performs surprisingly well in many domains containing clear attribute dependences suggest that the answer to this question may be positive. This article shows that, although the Bayesian classifier's probability estimates are only optimal under quadratic loss if the independence assumption holds, the classifier itself can be optimal under zero-one loss (misclassification rate) even when this assumption is violated by a wide margin. The region of quadratic-loss optimality of the Bayesian classifier is in fact a second-order infinitesimal fraction of the region of zero-one optimality. This implies that the Bayesian classifier has a much greater range of applicability than previously thought. For example, in this article it is shown to be optimal for learning conjunctions and disjunctions, even though they violate the independence assumption. Further, studies in artificial domains show that it will often outperform more powerful classifiers for common training set sizes and numbers of attributes, even if its bias is a priori much less appropriate to the domain. This article's results also imply that detecting attribute dependence is not necessarily the best way to extend the Bayesian classifier, and this is also verified empirically.},
	language = {en},
	number = {2},
	urldate = {2022-03-09},
	journal = {Machine Learning},
	author = {Domingos, Pedro and Pazzani, Michael},
	month = nov,
	year = {1997},
	pages = {103--130},
}

@book{Lu2011,
	address = {Piscataway, NJ},
	title = {Multi-aspect {Sentiment} {Analysis} with {Topic} {Models}},
	isbn = {9781467300056},
	shorttitle = {2011 {IEEE} 11th {International} {Conference} on {Data} {Mining} workshops ({ICDMW} 2011)},
	language = {eng},
	publisher = {IEEE},
	author = {Lu, Bin and Ott, Myle and Cardie, Claire and Tsou, Benjamin},
	editor = {Spiliopoulou, Myra and {Institute of Electrical and Electronics Engineers} and {IEEE Computer Society}},
	year = {2011},
}

@book{Hitzler2019,
	address = {Cham, Switzerland},
	series = {Lecture notes in computer science},
	title = {The semantic web: 16th international conference, {ESWC} 2019, {Portorož}, {Slovenia}, {June} 2-6, 2019: proceedings},
	isbn = {9783030213480 9783030213473},
	shorttitle = {The semantic web},
	language = {eng},
	number = {11503},
	publisher = {Springer},
	editor = {Hitzler, Pascal},
	year = {2019},
}
