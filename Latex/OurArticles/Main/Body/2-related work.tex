\section{Related Work}
\label{sec:literature}
This section discusses the literature related to our research. Methods for addressing ABSC are classified into three types: knowledge-based methods, machine learning methods, and hybrid approaches \cite{brauwers2021}. The most important findings for key methods within these categories are discussed, building up to a brief summary of LCR-Rot-hop++. Last, results of previous works using document knowledge transfer are analysed.

\subsection{ABSC Models}

Traditionally, dictionary-based approaches have represented the foundation for ABSC. One of the first implementations of such an approach was presented in \cite{Hu2004}. In this method, a basic collection of words is manually labeled. Next, a synonym/antonym graph is used, to further classify words. To illustrate, a synonym of a word with a positive sentiment will be labeled positive as well.

Dictionary-based approaches are, however, manually intensive, which has led to an increase in the popularity of machine learning for ABSC. BiLSTMs, specifically, form the basis of many advanced models for ABSC. More precisely, over the years, extensions of BiLSTMs have been developed to better account for the context surrounding the aspect. One of the more popular such extensions is LCR-Rot, which utilizes left-center-right separated LSTMs as well as a rotatory attention mechanism to create separate target representations for the aspect itself, the context to the left, and the context to the right \cite{Zheng2018}. After that, the model applies a classification head over the concatenation of their representations, and has been found to outperform several other machine learning models \cite{Zheng2018}. 

% The first such approaches involved standard algorithms such as Na\"{\i}ve Bayes, Maximum Entropy classification, and Support Vector Machines were popular \cite{Pang2002}, but more recently, neural networks have been applied to the field of NLP \cite{Do2019}. Some examples are Convoluted Neural Networks (CNNs), which have been shown to be successful in many NLP tasks, including ABSC \cite{Kim2014}, as well as Recurrent Neural Networks (RNNs), which utilise hidden states to capture context dependencies. These methods, however, suffer from the vanishing gradient problem, leading to short-term memory, which drove the development and use of LSTMs \cite{Hochreiter1997}. 

The LCR-Rot model has been further extended in many directions, most notably with respect to the attention mechanism and the word embeddings. In particular, \cite{Wallaart2019} proposes an LCR-Rot-hop model that iteratively applies the rotatory attention mechanism proposed in \cite{Zheng2018}, which outperforms the base model as it can better capture sentiment presented in a complex sentence. In \cite{Trusca2020}, the LCR-Rot-hop++ model is proposed as a further extension of this, by using a hierarchical attention mechanism and deep contextual word embeddings. The advantage of using hierarchical attention is that the relevance of the attention vectors is calculated also at a sentence level next to the word-level attention. Deep contextual word embeddings are useful in that they take into account the context of words and thus can better handle the semantics of a sentence. These two additions to the LCR-Rot-hop model have been shown to increase prediction accuracy for two standard datasets \cite{SemEval2015, SemEval2016}. 

Dictionary-based approaches and machine learning models are often combined into hybrid models for ABSC. Two-step classification methods have been proven to be particularly effective compared to combined models  \cite{Cambria2018,Chikersal2015,Schouten2016,Schouten2017,Wallaart2019}. One of the first two-step approaches was proposed in \cite{Chikersal2015}, and improvements on this method are proposed in a sequence of works \cite{Schouten2016,Schouten2017,Wallaart2019} followed by \cite{Trusca2020}. \cite{Trusca2020} presents HAABSA++, where a domain ontology is used to determine the sentiment for each sentence and LCR-Rot-hop++ is applied to any sentences for which the ontology is inconclusive, also leading to better performance than solely using LCR-Rot-hop++.

\subsection{Document Knowledge Transfer} 

The performance of the LCR-Rot-hop++ model depends on the scale of the available training data, as limited training data can lead to a lower accuracy. Ideally, one would use aspect-level data, as the model is used for sentiment analysis at the aspect level. However, the availability of annotated aspect-level data is limited \cite{pan2010, He2018}. To illustrate, both \cite{Trusca2020} and \cite{Wallaart2019} use the standard SemEval 2015 and SemEval 2016 datasets \cite{SemEval2015, SemEval2016}, which are relatively small. Due to the limited availability of aspect-level data, the LCR-Rot-hop++ model may not reach its full potential. To overcome this issue, one can consider coarser data, such as document level or sentence level data. There is an abundance of this type of data, for instance Yelp reviews \cite{Tang2015}. 

Document knowledge transfer can be motivated from three perspectives: human learning, pedagogy, and machine learning \cite{Ruder2019}. From the point of view of human learning, it is clear that we frequently use knowledge acquired from learning related tasks when learning a new task. Equally, from a pedagogical perspective, we often learn the foundations first, before using this knowledge to learn more complex skills. Last, document knowledge transfer improves generalization by introducing an inductive bias, which creates a preference for hypotheses that explain more than one task \cite{Caruana1993}. In this paper, we investigate which method for document transfer knowledge performs best, specifically, we consider combinations of pretraining (PRET), multi-task learning (MULT), and fine-tuning (FT). 

\subsubsection{PRET.}
Pretraining is the act of training a model on a task semantically related to your target task, prior to training for your target task \cite{Ruder2019, He2018}. This technique has shown great success in language models such as BERT in \cite{Devlin2019}. BERT was trained to perform two tasks which helped the model understand language, after which it can be fine-tuned for a wider variety of language tasks. As shown in \cite{He2018}, pretraining a BiLSTM on document-level data improved the results obtained on an aspect level. 

\subsubsection{MULT.}
In contrast to PRET, when using MULT the model is trained for the target task and the semantically related task simultaneously \cite{Ruder2019}. The purpose of this is to improve generalization, which might lead to more effective knowledge transfer. For example, \cite{Subramanian2018} demonstrated that multi-task learning is able to produce good word embeddings.

\subsubsection{FT.}
In PRET and MULT, one uses the semantically-related task to improve performance on the target task. With FT, one only trains on the target task. Therefore, it is necessary in combination with PRET, but optional with MULT. To illustrate, the BERT language model first gets trained using a MULT approach on general language tasks, after which it can be trained for specific tasks using an FT approach \cite{Devlin2019}.

% Three types of methods are classified for addressing ABSA: knowledge-based methods, machine learning methods, and a hybrid approach \cite{schouten_survey_2016}. Earlier methods exploit (implicit) rules in human language. For example, aspects were detected by its frequency \todo{citation needed}. Some extracted sentiment by assigning scores to words in the dictionary \todo{citation needed}. While others exploited syntax rules such as adjectives usually being close to nouns \todo{citation needed} and negations \todo{citation needed}. However, some drawbacks of these methods are the dependence on linguistic correctness \todo{citation needed} and the manual labor required to implement these rules \cite{bielikova_hybrid_2020}. 

% Therefore, machine learning methods were investigated. Early successful models include Maximum Entropy, Conditional Random Field, and Support Vector Machine methods \todo{citations needed}. These were further developed to incorporate neural models such as GRU \todo{this one im not 100\% sure about} and Long Short-Term Memory models \todo{maybe other methods are better suited}. While these methods are quite efficient and more robust to linguistic errors, they still require manual labor to label the large training dataset. Furthermore, results might differ across domains\todo{citation needed}. For example, a hot environment might be preferable in a spa resort but not for a ski resort.

% It turns out that combining both approaches sequentially further improved results, developing into the two-step Hybrid Approach for Aspect-Based Sentiment Analysis \todo{citation needed}. The first step is to determine the sentiment using domain ontology \todo{citation needed, also is domain ontology general? bc i do not mention LCR next}. The second step gathers all inconclusive sentiments and employs machine learning techniques to determine sentiment.

% \todo[inline]{Other direction of improving HAABSA: document knowledge transfer (adds document level sentiment analysis)}

% Another method to improve performance is to incorporate document-level sentiment analysis \cite{he_exploiting_2018}. Knowledge from a lower level is added and is less laborious to obtain, meaning that less aspect-level data is needed. \todo{talk about PRET, MULT and PRET+MULT}

% \todo[inline]{Combine both advancements}

% We explore the advancements of \cite{bielikova_hybrid_2020, he_exploiting_2018}, by combining both lines of thought. Our hypothesis is that document knowledge transfer will improve the performance of the advanced neural network.