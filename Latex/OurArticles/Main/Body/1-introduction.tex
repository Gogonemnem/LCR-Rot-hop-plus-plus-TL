\section{Introduction}
% \label{sec:introduction}

% \todo[inline]{Formulation \textit{of the research problem}.

% Motivation: \textit{why actually is the research problem a ‘problem’? Why is our current knowledge of the topic insufficient? Why is further research required?}

% Relevance: \textit{why and for whom is the ‘problem’ important?}}

In the pre-Web era, it was often difficult for companies to gauge the opinions of their large customer bases. While the increasing popularity of the Web provided a virtually inexhaustible source of data, machine learning methods had to be developed for extracting insights from this information. One particularly interesting insight is to extract a sentiment from a segment of text, for example a review. This is what drove the development of Sentiment Analysis in the field of Natural Language Processing (NLP) \cite{Liu2020}. 

This paper specifically considers Aspect-Based Sentiment Analysis (ABSA). ABSA consists of two steps: Aspect Detection (AD) and Aspect-Based Sentiment Classification (ABSC). AD is the task of finding an aspect, such as price, quality, or service of an entity, within a text or review. This paper will focus on ABSC exclusively, which involves determining the sentiment of a given aspect within a given sentence\cite{brauwers2021, Schouten2017}. It is common practice to divide sentiment into three classes: positive, neutral, and negative.

Traditionally, dictionary-based approaches such as that in \cite{Hu2004} have been used, but with the rise of Deep Learning and the ever-increasing computational power of modern machines, a range of new techniques for ABSC have become available. Of the basic ``deep'' models, the Bidirectional Long Short-Term Memory Network (BiLSTM) at first appeared to be the most promising, as illustrated in \cite{Graves2005}. Over the years, however, more sophisticated BiLSTM methods have been developed. One of which is the Left-Center-Right Separated BiLSTM with Rotatory Attention (LCR-Rot) \cite{Zheng2018}, which utilizes separately trained BiLSTM networks for the context to the left of the aspect, the aspect itself, and the context to the right of the aspect, and has been found to outperform previously proposed LSTM-variations \cite{Zheng2018}. Even more recently, the LCR-Rot model has been extended with respect to both the attention mechanism and the word embeddings. Namely, the LCR-Rot-hop model presented in \cite{Wallaart2019} iteratively applies the attention mechanism, while the LCR-Rot-hop++ model proposed in \cite{Trusca2020} builds on this to include hierarchical attention and deep contextual word embeddings. 

Over the past years, a collection of techniques called Transfer Learning (TL) has surged in popularity as a method to improve the performance of machine learning methods. More formally, Transfer Learning involves training a model on auxiliary tasks to improve the performance for our main task. This is particularly interesting when there is few data available for our task at hand. One such method is multi-task learning (MULT), where a model is trained on two tasks simultaneously, as applied in the widely used language model called Bidirectional Encoder Representation from Transformers (BERT) \cite{Devlin2019}. An alternative method is pretraining (PRET), which involves first learning an auxiliary task after which the model is trained for the main task. The latter step is called fine-tuning (FT), meaning a TL model is trained once more on just the main task.

A lack of training data in the same domain as the test data is a persistent issue in machine learning \cite{pan2010}. In ABSC, this is reflected by the limited availability of aspect-level data. As there is more annotated sentiment data available at a document level, i.e. review texts with star ratings, this information can be exploited using TL techniques. \cite{He2018}, for example, showed an improvement in the performance of ABSC in BiLSTMs when PRET and MULT are utilized. We consider four approaches for document knowledge transfer in the LCR-Rot-hop++ model, inspired by \cite{He2018}. The first approach is PRET+FT, in which the model is first pretrained on the document knowledge and then fine-tuned. The second is MULT, where the sentiment of a document and of an aspect are determined simultaneously. While the method proposed in \cite{He2018} does not include a regularization term, we extend the approach by including a regularization term in the loss function as in \cite{Wallaart2019}. The third method is a combination of both PRET and MULT, called PRET+MULT, in which the model is first pretrained at a document level on part of the data, before MULT is applied to the rest of the data. Last, in the fourth and fifth approaches, we develop new methods that incorporate FT into the TL approach, in two models called MULT+FT and PRET+MULT+FT. 

The present work extends the existing literature by implementing document knowledge transfer on the state-of-the-art LCR-Rot-hop++ model, with the aim to further improve its accuracy. Moreover, different L1 and L2 regularisation terms are combined to improve upon the previous works. Additionally, it is analysed how the results for multi-task learning change when the emphasis on the auxiliary task is altered. Last, different sizes of pretraining corpora are used to investigate the effect on the accuracy of our main task. The Python source code of our models can be found at \url{https://github.com/Gogonemnem/seminar-ba-qm}.

The paper continues as follows. First, the related works and their results are discussed in more detail in Sect. 2, after which the data is illustrated in Sect. 3. Subsequently, the methodology is presented in Sect. 4, in which the LCR-Rot-hop++ model and the experimental setup are explained thoroughly. Thereafter, the results are compared with those obtained in the previous literature in Sect. 5. Last, conclusions with the main findings and suggestions for future research are presented in Sect. 6.

% PAST INTRODUCTION

% In the pre-Web era, companies often struggled to gauge the opinions of their large customer bases. Likewise, governments had difficulties in determining the public opinion, which complicated policy-making. The increasing popularity of the Web lead to a virtually inexhaustible source of data, for which manual processing is impossible. Consequently, machine learning methods were developed for extracting information from the data. One of these challenges was to extract a sentiment, either positive, neutral or negative, from a segment of text. The abundance of freely provided opinions is what drove the development of Sentiment Analysis in the field of Natural Language Processing (NLP) \cite{Liu2020}. This paper focuses specifically on Aspect-Based Sentiment Analysis (ABSA), which aims to identify the sentiment towards a given aspect of a product, for example its quality \cite{Schouten2016}.

% ABSA consists of two steps: Aspect Detection (AD) and Aspect-Based Sentiment Classification (ABSC). AD is the task of finding an aspect, such as price, quality, or service of an entity, within a text or review. As illustrated in \cite{Schouten2016}, a wide variety of techniques for AD are available; some focus merely on the frequency of words, whereas others are more complex and exploit machine learning techniques. This paper, however, will focus on ABSC exclusively, hence the models discussed in this work thus focus on determining the sentiment of a given aspect within a given sentence. It is common practice to divide sentiment into three classes: positive, neutral, and negative.

% Traditionally, dictionary-based approaches such as that in \cite{Hu2004} have been used, but with the rise of Deep Learning methods and the ever-increasing computational power of modern machines, a range of new techniques for ABSC have become available. Of the basic "deep" models, the Bidirectional Long Short-Term Memory Network (BiLSTM) at first appeared to be the most promising, as illustrated in \cite{Graves2005}. Over the years, however, more sophisticated BiLSTM methods have been developed, one of which is the Left-Center-Right Separated BiLSTM with Rotatory Attention (LCR-Rot) \cite{Zheng2018}. This model utilizes separately trained BiLSTM networks for the context to the left of the aspect, the aspect itself, and the context to the right of the aspect, and has been found to outperform previously proposed LSTM-variations \cite{Zheng2018}. Even more recently, the LCR-Rot model has been extended with respect to both the attention mechanism and the word embeddings. Namely, the LCR-Rot-hop model presented in \cite{Wallaart2019} iteratively applies the attention mechanism, while the LCR-Rot-hop++ model proposed in \cite{Trusca2020} builds on this to include hierarchical attention and deep contextual word embeddings. 

% Over the past years, a collection of techniques called Transfer Learning has surged in popularity due to its effectiveness. More formally, Transfer Learning is the idea to train a model on auxiliary tasks to improve the performance for our main task. The widely used language model, Bidirectional Encoder Representation from Transformers (BERT) \cite{Devlin2019}, is trained on two tasks simultaneously, which is called multi-task learning (MULT). Learning two semantically related tasks can provide positive external effects in their performance as it leads to shared layers within the model. Instead of learning them at the same time, one could also first learn the auxiliary task before learning the main task, which is referred to as pretraining (PRET). After the PRET and/or MULT phases are finished, a model can be trained on our main task specifically, which is called fine-tuning (FT).

% A persistent issue of ABSC is the limited availability of data on such a specific aspect-level. More generally, the lack of training data in the same domain as the test data is a common issue in machine learning problems \cite{pan2010}. Recently, the idea of document knowledge transfer has been popularized by \cite{He2018}, which showed an improvement in the performance of aspect-level sentiment analysis in BiLSTMs. As there is more annotated sentiment data available at a document level, one can exploit this information using Transfer Learning techniques\todo{I still think he wants more discussion of transfer learning in general here, based on what he said in the meeting}. The present work extends the existing literature by implementing document knowledge transfer on the LCR-Rot-hop++ model, with the aim to improve its accuracy even further. More specifically, four approaches for document knowledge transfer will be considered. The first approach is known as pretraining, where the LCR-Rot-hop++ model is first pretrained on the document knowledge, with the idea that the model learns sentiment on a higher document-level. The second method for document knowledge transfer is multi-task learning, where the sentiment of a document and of an aspect are determined simultaneously. The third approach combines both methods: the model is first pretrained at a document level, before multi-task learning is applied. Last, in a fourth approach, a new transfer learning method is tested which combines pretraining, multi-task learning, and fine-tuning.

% This paper contributes to the existing literature in three ways. First, document knowledge transfer is applied to the state-of-the-art LCR-Rot-hop++ model. Secondly, fine-tuning is incorporated into the transfer learning approach, which has not been done in this setting. Finally, we regularize the loss function to avoid overfitting.

% The paper continues as follows. First, the related works and their results are discussed in more detail in Sect. 2, after which the data is illustrated in Sect. 3. Subsequently, the methodology is presented in Sect. 4, in which the LCR-Rot-hop++ model and the experimental setup are explained thoroughly. Thereafter, the results are compared with ones obtained in the previous literature for the same dataset in Sect. 5. Finally, a conclusion of the main findings and a discussion containing directions for future research are presented in Sect. 6.