\section{Conclusion}
ABSC models are constrained due to the limited availability of aspect-level training data. In this paper, we try to overcome this limitation by using document-level training data to train the state-of-the-art LCR-Rot-Hop++ model. The results show that the most successful transfer learning approach is multi-task learning, particularly when faced with a small and imbalanced dataset such as SemEval 2015. Likely, multi-task learning outperforms the pretraining approach due to catastrophic forgetting; document knowledge acquired in pretraining is partly forgotten when the model is retrained on aspects. Multi-task learning solves this problem by fitting on the main and auxiliary task simultaneously, preventing this type of forgetting \cite{Chen2020}. 

Our best approach, the MULT model, yields a 2.2 percentage point increase relative to the state-of-the-art LCR-Rot-hop++ model with L1 and L2 regularisation for the SemEval 2015 dataset, as well as a 0.4 percentage point increase for the SemEval 2016 dataset. Furthermore, this model performs equally well as the HAABSA++ model for the SemEval 2015 dataset and better than the HAABSA++ model for the SemEval 2016 dataset. Therefore, we conclude that the inclusion of L1 and L2 regularisation terms along with the MULT method of document knowledge transfer can effectively compensate for the exclusion of an ontology step. Hence, this updated model can serve as a computationally cheaper alternative to existing hybrid models, without any significant loss in performance.

A suggestion for future research is to investigate different deep learning architectures for incorporating document knowledge transfer. One example of a different architecture is adding a shared BiLSTM layer below the LCR-Rot-hop++ model, instead of sharing the left, middle and right BiLSTM. Furthermore, future research can investigate models that exploit sentence or paragraph level knowledge, besides document level knowledge.  