import tensorflow as tf
from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Activation, Dropout
from tensorflow_addons.layers import AdaptiveAveragePooling1D
from embedding import GloveEmbedding, BERTEmbedding
from attention import BilinearAttention, HierarchicalAttention


class TL(tf.keras.Model):
    def __init__(self, data_paths: list = None, embedding_path: str = None, invert: bool = False, hop: int = 1, hierarchy: tuple = None, drop_1: float = 0.2, drop_2: float = 0.5, hidden_units: int = None, regularizer=None):
        """Creates a new Transfer Learning model described in the paper.

        Args:
            data_paths (list, optional): List of paths to the training & validation data for GloVe Embeddings. Defaults to None.
            embedding_path (str, optional): Path to the GloVe embeddings. Defaults to None.
            invert (bool, optional): If true, invert the order of 2-step approach. Defaults to False (Target2Context then Context2Target).
            hop (int, optional): Number of hops. Defaults to 1. If 0 hops, there is no attention layer applied.
            hierarchy (tuple, optional): Tuple which decides which of the 4 hierarchical attention methods is used. First item: whether to combine the softmaxlayers. Second item: whether to apply the layer iteratively. Defaults to None, no hierarchical attention used.
            drop_1 (float, optional): Float between 0 and 1. Fraction of the input units to drop. Defaults to 0.2.
            drop_2 (float, optional): Float between 0 and 1. Fraction of the output neurons to drop. Defaults to 0.5.
            hidden_units (int, optional): Number of hidden units in the LSTM layer. Thus half the number of hidden units in the BiLSTM layer. If None, number is equal to the embedding dimension. Defaults to None.
            regularizer (_type_, optional): A tensorflow/keras regularizer. E.g., L1, L2 or L1 & L2. Defaults to no regularizer.

        Make sure to use sensible inputs, no checks are made.
        """
        super().__init__()

        # Determines which of the described methods in the papers from Wallaart & Trusca
        self.invert = invert
        self.hop = hop
        self.hierarchy = hierarchy

        # According to https://arxiv.org/pdf/1207.0580.pdf
        self.drop_input = Dropout(drop_1)
        self.drop_output = Dropout(drop_2)

        # Check which embedding to use
        if data_paths and embedding_path:
            self.embedding = GloveEmbedding(
                embedding_path, data_paths)
        else:
            self.embedding = BERTEmbedding()
        self.embedding_dim = self.embedding.embedding_dim

        # BiLSTM layers
        self.hidden_units = hidden_units if hidden_units else self.embedding_dim
        self.left_bilstm = Bidirectional(
            LSTM(self.hidden_units, return_sequences=True))
        self.target_bilstm = Bidirectional(
            LSTM(self.hidden_units, return_sequences=True))
        self.right_bilstm = Bidirectional(
            LSTM(self.hidden_units, return_sequences=True))

        # Attention layers
        self.average_pooling = AdaptiveAveragePooling1D(1)

        self.attention_left = BilinearAttention(
            2*self.hidden_units, regularizer)
        self.attention_right = BilinearAttention(
            2*self.hidden_units, regularizer)
        self.attention_target_left = BilinearAttention(
            2*self.hidden_units, regularizer)
        self.attention_target_right = BilinearAttention(
            2*self.hidden_units, regularizer)

        # Hierarchical attention layers
        if hierarchy is not None:
            if hierarchy[0]:  # combine all
                self.hierarchical = HierarchicalAttention(
                    2*self.hidden_units, regularizer)
            else:  # separate inner & outer
                self.hierarchical_inner = HierarchicalAttention(
                    2*self.hidden_units, regularizer)
                self.hierarchical_outer = HierarchicalAttention(
                    2*self.hidden_units, regularizer)

        # MLP layer, why is it called MLP btw? I donÂ´t think it should be.
        self.doc_dense = Dense(3, Activation(
            'softmax'), bias_initializer='zeros', kernel_regularizer=regularizer, bias_regularizer=regularizer)
        self.asp_dense = Dense(3, Activation(
            'softmax'), bias_initializer='zeros', kernel_regularizer=regularizer, bias_regularizer=regularizer)

    def call(self, inputs):
        """Describes the model by relating the layers

        Args:
            inputs: List of document data, the left context, target, right context

        Returns: Probabilities per class per data level
        """
        # Separate inputs: LCR
        input_document, input_left, input_target, input_right = inputs[
            0], inputs[1], inputs[2], inputs[3]

        ### Embedding & BiLSTMs
        # Document level
        if input_document is not None: # check if documents are present
            embedded_document = self.embedding(input_document)
            embedded_document = self.drop_input(embedded_document)

            doc_left_bilstm = self.left_bilstm(embedded_document)
            doc_center_bilstm = self.target_bilstm(embedded_document)
            doc_right_bilstm = self.right_bilstm(embedded_document)

            doc_left = self.average_pooling(doc_left_bilstm)
            doc_center = self.average_pooling(doc_center_bilstm)
            doc_right = self.average_pooling(doc_right_bilstm)

        # Aspect level
        if not None in [input_left, input_target, input_right]: # inputs[0:3], check if aspects are present
            # embedding is not trainable, thus you can use it again
            embedded_left = self.embedding(input_left)
            embedded_left = self.drop_input(embedded_left)
            left_bilstm = self.left_bilstm(embedded_left)

            embedded_target = self.embedding(input_target)
            embedded_target = self.drop_input(embedded_target)
            target_bilstm = self.target_bilstm(embedded_target)

            embedded_right = self.embedding(input_right)
            embedded_right = self.drop_input(embedded_right)
            right_bilstm = self.right_bilstm(embedded_right)

            # Representations
            representation_target_left = representation_target_right = self.average_pooling(
                target_bilstm)[:, 0, :]

            representation_left = self.average_pooling(left_bilstm)[:, 0, :]
            representation_right = self.average_pooling(right_bilstm)[:, 0, :]

            # Attention layers
            # for hop == 0, this loop is skipped -> LCR model (no attention, no rot)
            for _ in range(self.hop):
                representation_left, representation_target_left, representation_target_right, representation_right = self._apply_bilinear_attention(
                    left_bilstm, target_bilstm, right_bilstm, representation_left, representation_target_left, representation_target_right, representation_right)

                if self.hierarchy is not None and self.hierarchy[1]:
                    representation_left, representation_target_left, representation_target_right, representation_right = self._apply_hierarchical_attention(
                        representation_left, representation_target_left, representation_target_right, representation_right)

            if self.hierarchy is not None and (not self.hierarchy[1] or self.hop == 0):
                representation_left, representation_target_left, representation_target_right, representation_right = self._apply_hierarchical_attention(
                    representation_left, representation_target_left, representation_target_right, representation_right)

        # MLP
        # Document level
        if input_document is not None: # check if documents are present
            v = tf.concat([doc_left, doc_center, doc_right], axis=1)
            v = self.drop_output(v)

            doc_pred = self.doc_dense(v)

        # Aspect level
        if not None in [input_left, input_target, input_right]: # inputs[0:3], check if aspects are present
            v = tf.concat([representation_left, representation_target_left,
                        representation_target_right, representation_right], axis=1)
            v = self.drop_output(v)

            asp_pred = self.asp_dense(v)

        return [doc_pred, asp_pred]

    def _apply_bilinear_attention(self, left_bilstm, target_bilstm, right_bilstm, representation_left, representation_target_left, representation_target_right, representation_right):
        """Applies the attention layer described by in the paper"""
        if self.invert:
            representation_target_left = self.attention_target_left(
                [target_bilstm, representation_left])
            representation_target_right = self.attention_target_right(
                [target_bilstm, representation_right])

        representation_left = self.attention_left(
            [left_bilstm, representation_target_left])
        representation_right = self.attention_right(
            [right_bilstm, representation_target_right])

        if not self.invert:
            representation_target_left = self.attention_target_left(
                [target_bilstm, representation_left])
            representation_target_right = self.attention_target_right(
                [target_bilstm, representation_right])

        return representation_left, representation_target_left, representation_target_right, representation_right

    def _apply_hierarchical_attention(self, representation_left, representation_target_left, representation_target_right, representation_right):
        """Applies the hierarchical attention layer described by in the paper"""
        if self.hierarchy[0]:  # combine all
            representations = tf.stack(
                [representation_left, representation_target_left, representation_target_right, representation_right], axis=1)
            representation_left, representation_target_left, representation_target_right, representation_right = tf.unstack(
                self.hierarchical(representations), axis=1)
        else:  # separate inner & outer
            representations = tf.stack(
                [representation_left, representation_right], axis=1)
            representation_left, representation_right = tf.unstack(
                self.hierarchical_outer(representations), axis=1)

            representations = tf.stack(
                [representation_target_left, representation_target_right], axis=1)
            representation_target_left, representation_target_right = tf.unstack(
                self.hierarchical_inner(representations), axis=1)
        return representation_left, representation_target_left, representation_target_right, representation_right
